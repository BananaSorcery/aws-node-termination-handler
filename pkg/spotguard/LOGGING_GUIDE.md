# Spot Guard Logging Guide

## Overview

This document explains all the debug and info logs generated by Spot Guard, making it easy to debug and monitor the system.

## Log Levels

- **INFO**: Important state changes and successful operations
- **DEBUG**: Detailed information for troubleshooting
- **WARN**: Non-fatal issues that should be investigated
- **ERROR**: Fatal errors that prevent operations

## Log Structure

All logs use structured logging (zerolog) with consistent fields:

```json
{
  "level": "info",
  "time": "2025-01-15T10:30:00Z",
  "asg": "my-spot-asg",
  "node": "ip-10-0-1-100",
  "instanceID": "i-1234567890abcdef0",
  "eventID": "fallback-i-1234-1234567890",
  "message": "Starting on-demand node scale-down operation"
}
```

## Log Flow: Complete Lifecycle

### 1. Fallback Event Recorded

```
INFO  Tracking new fallback event
      eventID=fallback-i-1234-1234567890
      onDemandASG=my-ondemand-asg
      spotASG=my-spot-asg
      onDemandNode=ip-10-0-1-100
```

**When**: On-demand instance scaled up due to spot capacity unavailability

**Fields**:
- `eventID`: Unique identifier for this fallback event
- `onDemandASG`: Name of on-demand Auto Scaling Group
- `spotASG`: Name of spot Auto Scaling Group
- `onDemandNode`: Kubernetes node name
- `instanceID`: EC2 instance ID (if available)

### 2. Monitor Loop Starts Checking

```
DEBUG Checking for scale-down opportunities
      activeEvents=1
```

**When**: Every 30 seconds (configurable)

**Fields**:
- `activeEvents`: Number of events being monitored

### 3. Minimum Wait Time Check

```
DEBUG Cannot scale down yet
      eventID=fallback-i-1234-1234567890
      reason="minimum wait time not met: 5m30s remaining"
```

**When**: Before minimum wait time has passed

**Fields**:
- `eventID`: Event being checked
- `reason`: Why scale-down is not ready

### 4. Spot Health Checks Begin

#### 4a. Checking Spot ASG Health

```
DEBUG Checking spot ASG health
      asg=my-spot-asg

DEBUG Spot ASG health check result
      asg=my-spot-asg
      inService=2
      desired=3
      min=1
      max=10
      pending=1
      unhealthy=0
      terminating=0
      totalInstances=3
      instances=[
        "i-aaa:InService:Healthy",
        "i-bbb:InService:Healthy",
        "i-ccc:Pending:Healthy"
      ]
      healthy=false
```

**When**: Checking if spot ASG has enough healthy instances

**Fields**:
- `asg`: Auto Scaling Group name
- `inService`: Count of InService+Healthy instances
- `desired`: Desired capacity
- `min/max`: ASG limits
- `pending/unhealthy/terminating`: Instance state counts
- `instances`: Detailed list of instances (format: `instanceID:state:health`)
- `healthy`: Boolean result

#### 4b. Checking Kubernetes Nodes

```
DEBUG Checking if spot nodes are ready in Kubernetes
      asg=my-spot-asg

DEBUG Listed all nodes in cluster
      asg=my-spot-asg
      totalNodesInCluster=15

DEBUG Found spot node via EKS label
      node=ip-10-0-2-50
      asg=my-spot-asg
      labelKey="eks.amazonaws.com/nodegroup"

DEBUG All spot nodes are ready and schedulable
      asg=my-spot-asg
      nodeCount=3
      nodeStatuses=[
        "ip-10-0-2-50:Ready",
        "ip-10-0-2-51:Ready",
        "ip-10-0-2-52:Ready"
      ]
```

**When**: Verifying spot nodes are ready in Kubernetes

**Fields**:
- `asg`: Auto Scaling Group name
- `totalNodesInCluster`: Total nodes found
- `node`: Individual node name
- `labelKey`: Which label was used to identify the node
- `nodeCount`: Number of spot nodes found
- `nodeStatuses`: Status of each node

#### 4c. Spot Capacity Becomes Healthy

```
INFO  Spot capacity became healthy (ASG + K8s), starting stability timer
      asg=my-spot-asg
      since=2025-01-15T10:35:00Z
      requiredStability=2m
```

**When**: First time spot is detected as healthy

**Fields**:
- `asg`: Auto Scaling Group name
- `since`: When the stability timer started
- `requiredStability`: How long it needs to stay healthy

#### 4d. Waiting for Stability

```
DEBUG Spot capacity is healthy but not yet stable for required duration
      asg=my-spot-asg
      elapsed=1m30s
      required=2m
      remaining=30s
```

**When**: Spot is healthy but hasn't been stable long enough

**Fields**:
- `elapsed`: Time since became healthy
- `required`: Required stability duration
- `remaining`: Time remaining until stable

#### 4e. Spot Capacity Stable

```
INFO  Spot capacity is stable for required duration
      asg=my-spot-asg
      elapsed=2m5s
      required=2m
```

**When**: Spot has been healthy for required duration

### 5. Pod Safety Check

```
DEBUG Starting pod safety check for node drain
      node=ip-10-0-1-100

DEBUG Retrieved node details
      node=ip-10-0-1-100
      instanceType=m5.xlarge
      zone=us-east-1a

DEBUG Checking pod safety for node drain
      node=ip-10-0-1-100
      totalPods=15

DEBUG Skipping DaemonSet pod (will be recreated on other nodes)
      node=ip-10-0-1-100
      pod=kube-system/aws-node-abc123

DEBUG Checking if pod can be safely evicted
      node=ip-10-0-1-100
      pod=default/my-app-xyz789
      phase=Running

DEBUG Pod can be safely evicted
      node=ip-10-0-1-100
      pod=default/my-app-xyz789

INFO  Pod safety check passed - all pods can be safely rescheduled
      node=ip-10-0-1-100
      totalPods=15
      daemonSets=5
      alreadyTerminating=2
      checked=8
```

**When**: Checking if pods can be safely evicted

**Fields**:
- `node`: Node being checked
- `instanceType/zone`: Node metadata
- `totalPods`: Total pods on node
- `pod`: Individual pod being checked
- `phase`: Pod phase (Running, Pending, etc.)
- `daemonSets/alreadyTerminating/checked`: Pod counts by category

### 6. Scale-Down Execution Begins

```
INFO  Executing on-demand scale-down
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100
      spotASG=my-spot-asg
      onDemandASG=my-ondemand-asg
      timeSinceFailover=15m30s

INFO  Starting on-demand node scale-down operation
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100
      instanceID=i-1234567890abcdef0
      onDemandASG=my-ondemand-asg
      spotASG=my-spot-asg
      onDemandRuntime=15m30s
```

**When**: All conditions met, starting scale-down

**Fields**:
- `eventID`: Event being processed
- `node`: Node being scaled down
- `instanceID`: EC2 instance ID
- `spotASG/onDemandASG`: ASG names
- `timeSinceFailover/onDemandRuntime`: How long on-demand has been running

### 7. Scale-Down Steps

#### Step 1: Tainting

```
INFO  Step 1/5: Tainting node
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100

DEBUG Getting node to apply taint
      node=ip-10-0-1-100

DEBUG Applying taint to node
      node=ip-10-0-1-100
      taintKey=spotguard/scale-down-pending
      taintValue=true
      effect=NoSchedule

INFO  Successfully applied scale-down taint to node
      node=ip-10-0-1-100
```

#### Step 2: Cordoning

```
INFO  Step 2/5: Cordoning node
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100

DEBUG Cordoning node (marking unschedulable)
      node=ip-10-0-1-100

INFO  Successfully cordoned on-demand node
      node=ip-10-0-1-100
```

#### Step 3: Draining

```
INFO  Step 3/5: Draining node (evicting pods)
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100

DEBUG Starting node drain (graceful pod eviction)
      node=ip-10-0-1-100
      timeout=5m

INFO  Successfully drained on-demand node
      node=ip-10-0-1-100
```

#### Step 4: Waiting for Pods

```
INFO  Step 4/5: Waiting for pods to be rescheduled
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100

DEBUG Waiting for pods to be evicted
      node=ip-10-0-1-100
      remainingPods=3

DEBUG Waiting for pods to be evicted
      node=ip-10-0-1-100
      remainingPods=0

INFO  All pods have been evicted from node
      node=ip-10-0-1-100
```

#### Step 5: Scaling Down ASG

```
INFO  Step 5/5: Scaling down on-demand ASG
      eventID=fallback-i-1234-1234567890
      asg=my-ondemand-asg

DEBUG Getting current ASG capacity
      asg=my-ondemand-asg

DEBUG Current ASG capacity
      asg=my-ondemand-asg
      currentDesired=2
      currentInstances=2
      min=0
      max=10

INFO  Updating ASG desired capacity
      asg=my-ondemand-asg
      oldDesired=2
      newDesired=1

INFO  Successfully scaled down on-demand ASG
      asg=my-ondemand-asg
      oldCapacity=2
      newCapacity=1
```

### 8. Scale-Down Complete

```
INFO  Successfully completed on-demand node scale-down operation
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100
      instanceID=i-1234567890abcdef0
      onDemandASG=my-ondemand-asg
      totalRuntime=16m45s

INFO  Successfully completed on-demand scale-down
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100
      totalDuration=16m45s

INFO  On-demand instance runtime metric
      metric=ondemand_runtime_seconds
      value=1005.0
      spotASG=my-spot-asg
      onDemandASG=my-ondemand-asg
```

**When**: Scale-down operation completed successfully

**Fields**:
- `totalRuntime/totalDuration`: How long on-demand ran
- `value`: Runtime in seconds (for metrics)

## Error Scenarios

### Spot ASG Not Found

```
ERROR ASG not found
      asg=my-spot-asg
```

### Failed to Taint Node

```
ERROR Failed to taint node
      eventID=fallback-i-1234-1234567890
      node=ip-10-0-1-100
      error="node not found"
```

### PDB Would Be Violated

```
WARN  Evicting pod would violate PodDisruptionBudget
      node=ip-10-0-1-100
      pod=default/redis-master
      reason="PDB redis-pdb would be violated"
```

### Cannot Reschedule Pod

```
WARN  Pod cannot be rescheduled elsewhere
      node=ip-10-0-1-100
      pod=default/large-app
      reason="no suitable node found with sufficient resources"
```

### Spot Capacity Became Unhealthy

```
DEBUG Spot ASG became unhealthy, resetting stability timer
      asg=my-spot-asg
      wasHealthyFor=1m30s
```

### ASG At Minimum Capacity

```
ERROR Cannot decrease capacity below minimum size
      asg=my-ondemand-asg
      desiredNew=0
      minSize=1
```

## Filtering Logs

### View Only Spot Guard Logs

```bash
kubectl logs -n kube-system -l app=aws-node-termination-handler | grep -E "spotguard|spot guard"
```

### View Specific Event

```bash
kubectl logs -n kube-system -l app=aws-node-termination-handler | grep "eventID=fallback-i-1234-1234567890"
```

### View Only Errors

```bash
kubectl logs -n kube-system -l app=aws-node-termination-handler --tail=1000 | grep '"level":"error"'
```

### View Scale-Down Operations

```bash
kubectl logs -n kube-system -l app=aws-node-termination-handler | grep "scale-down"
```

### View Spot Health Checks

```bash
kubectl logs -n kube-system -l app=aws-node-termination-handler | grep "Spot ASG health"
```

### View Pod Safety Checks

```bash
kubectl logs -n kube-system -l app=aws-node-termination-handler | grep "pod safety"
```

## Typical Log Sequence

A successful scale-down will show this sequence:

```
1. INFO  Tracking new fallback event (on-demand scaled up)
2. DEBUG Checking for scale-down opportunities (every 30s)
3. DEBUG Cannot scale down yet (minimum wait)
4. DEBUG Spot ASG health check result (not healthy)
5. INFO  Spot capacity became healthy (timer starts)
6. DEBUG Spot capacity healthy but not yet stable
7. INFO  Spot capacity is stable
8. DEBUG Starting pod safety check
9. INFO  Pod safety check passed
10. INFO Executing on-demand scale-down
11. INFO Step 1/5: Tainting node
12. INFO Step 2/5: Cordoning node
13. INFO Step 3/5: Draining node
14. INFO Step 4/5: Waiting for pods
15. INFO Step 5/5: Scaling down ASG
16. INFO Successfully completed operation
```

## Troubleshooting with Logs

### Problem: On-demand not scaling down

**Check these logs**:
1. Is event being tracked? Look for "Tracking new fallback event"
2. Is minimum wait met? Look for "minimum wait time not met"
3. Is spot healthy? Look for "Spot ASG health check result"
4. Is spot stable? Look for "not yet stable for required duration"
5. Can pods reschedule? Look for "Pod safety check"

### Problem: Scale-down taking too long

**Check these logs**:
1. When did spot become healthy? Look for "starting stability timer"
2. How long is stability duration? Check "requiredStability" field
3. Are pods slow to evict? Look for "Waiting for pods" step

### Problem: Repeated failures

**Check these logs**:
1. Look for ERROR level logs
2. Check "reason" fields in WARN logs
3. Verify ASG names and instance IDs match

## Summary

All logs include relevant context (ASG names, instance IDs, node names, event IDs, errors) making it super easy to debug! Just grep for what you need! ðŸŽ‰

